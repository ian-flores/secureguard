---
title: "Advanced Guardrail Patterns"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Advanced Guardrail Patterns}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What This Vignette Covers

The getting-started vignette (`vignette("secureguard")`) introduces the three
defense layers and the built-in guardrails. This vignette goes deeper. It
covers building your own guardrails for domain-specific threats, composing
guardrails into layered defenses with fine-grained control over pass/fail
logic, assembling full pipelines that protect every stage of an agent turn,
and integrating with securer for sandboxed execution.

If you are new to secureguard, start with `vignette("secureguard")` first.

## How a Guardrail Pipeline Works

Before diving into custom guardrails, it helps to see how the pieces fit
together in a complete agent workflow. The following diagram shows the flow
of data through a `secure_pipeline()` -- the recommended way to wire
guardrails into an agent loop:

```
         User prompt
              |
              v
    +--------------------+
    |  check_input()     |    Input guardrails:
    |  - injection       |    prompt injection, topic scope, PII
    |  - topic scope     |
    |  - input PII       |
    +--------------------+
              |
         Pass? ----No----> Return failure (stage: input)
              |
             Yes
              |
              v
         LLM generates code
              |
              v
    +--------------------+
    |  check_code()      |    Code guardrails:
    |  - AST analysis    |    blocked functions, complexity,
    |  - complexity      |    dependencies, data flow
    |  - dependencies    |
    |  - data flow       |
    +--------------------+
              |
         Pass? ----No----> Return failure (stage: code)
              |
             Yes
              |
              v
      Execute in sandbox
        (securer)
              |
              v
    +--------------------+
    |  check_output()    |    Output guardrails:
    |  - PII             |    PII blocking, secret redaction,
    |  - secrets         |    size limits
    |  - size            |
    +--------------------+
              |
         Pass? ----No----> Return failure (stage: output)
              |
             Yes
              |
              v
      Return result to user
      (possibly redacted)
```

Each stage short-circuits on failure: if input guardrails reject the prompt,
the LLM never sees it. If code guardrails reject the generated code, it never
executes. This minimizes both risk and wasted computation.

## Creating Custom Guardrails

The built-in guardrails cover the most common threats: prompt injection, dangerous
function calls, PII leakage, and secret exposure. But every application has
domain-specific risks that generic guardrails cannot anticipate. An agent that
generates SQL needs SQL injection detection. A healthcare application needs
HIPAA-specific PII patterns. A financial tool needs checks for account numbers
and routing numbers.

secureguard is designed for extensibility. Every guardrail -- built-in or
custom -- is an S3 object of class `secureguard` with four properties: `name`,
`type`, `check_fn`, and `description`. The `new_guardrail()` constructor
validates these and returns a guardrail you can use with `run_guardrail()`,
`compose_guardrails()`, and `secure_pipeline()`. Custom guardrails compose
seamlessly with built-in ones because they share the same interface.

### A SQL Injection Detector

SQL injection is one of the oldest and most well-understood attack vectors in
software. When an LLM generates SQL queries, the risk is amplified: the model
may produce syntactically valid but semantically dangerous queries, especially
if the user's prompt contains adversarial patterns. A purpose-built guardrail
can catch these patterns before any query reaches a database.

```{r sql-injection-guard}
library(secureguard)

guard_sql_injection <- function() {
  sql_patterns <- c(
    "(?i)\\b(?:UNION\\s+SELECT|DROP\\s+TABLE|DELETE\\s+FROM)\\b",
    "(?i)\\b(?:INSERT\\s+INTO|UPDATE\\s+.+\\s+SET)\\b.*?;\\s*--",
    "(?i)'\\s*(?:OR|AND)\\s+['\"]?\\d['\"]?\\s*=\\s*['\"]?\\d",
    "(?i)(?:--|#|/\\*).*(?:SELECT|DROP|INSERT|UPDATE|DELETE)"
  )

  check_fn <- function(x) {
    hits <- vapply(sql_patterns, function(pat) {
      grepl(pat, x, perl = TRUE)
    }, logical(1))

    if (any(hits)) {
      guardrail_result(
        pass = FALSE,
        reason = "Potential SQL injection detected",
        details = list(
          matched_patterns = which(hits)
        )
      )
    } else {
      guardrail_result(pass = TRUE)
    }
  }

  new_guardrail(
    name = "sql_injection",
    type = "input",
    check_fn = check_fn,
    description = "Detects common SQL injection patterns"
  )
}
```

Now use it like any built-in guardrail:

```{r sql-injection-run}
g <- guard_sql_injection()
g

# Safe query
run_guardrail(g, "SELECT name FROM users WHERE id = 42")

# Injection attempt
run_guardrail(g, "SELECT * FROM users WHERE id = 1; DROP TABLE users; --")
```

### A Code Length Limiter

Custom guardrails of type `"code"` work exactly the same way. Here is one that
limits the number of lines in LLM-generated code:

```{r code-length-guard}
guard_code_length <- function(max_lines = 100L) {
  check_fn <- function(code) {
    n_lines <- length(strsplit(code, "\n", fixed = TRUE)[[1L]])
    if (n_lines > max_lines) {
      guardrail_result(
        pass = FALSE,
        reason = sprintf("Code has %d lines (max %d)", n_lines, max_lines),
        details = list(n_lines = n_lines, max_lines = max_lines)
      )
    } else {
      guardrail_result(pass = TRUE, details = list(n_lines = n_lines))
    }
  }

  new_guardrail(
    name = "code_length",
    type = "code",
    check_fn = check_fn,
    description = sprintf("Limits code to %d lines", max_lines)
  )
}

g_len <- guard_code_length(max_lines = 5)
run_guardrail(g_len, "x <- 1\ny <- 2\nz <- x + y")

long_code <- paste(sprintf("x%d <- %d", 1:10, 1:10), collapse = "\n")
run_guardrail(g_len, long_code)
```

### Anatomy of a check_fn

Every `check_fn` must:

1. Accept a single argument (the text or object to check).
2. Return a `guardrail_result()` with at minimum `pass = TRUE` or `pass = FALSE`.
3. Optionally include `reason` (why it failed), `warnings` (advisory notes),
   and `details` (a named list of metadata).

The `@` operator accesses properties on the result:

```{r result-properties}
result <- run_guardrail(guard_code_analysis(), "system('ls')")
result@pass
result@reason
result@details
```

## Composing Guardrails

Individual guardrails are building blocks. In practice, you almost always want
to run multiple guardrails together -- checking both for dangerous functions
and excessive complexity, or detecting both prompt injection and off-topic
prompts. secureguard provides two composition mechanisms that make this
natural.

### compose_guardrails(): Same-Type Composition

`compose_guardrails()` merges multiple guardrails of the **same type** into a
single composite guardrail. The result is itself a guardrail, which means you
can pass it to `run_guardrail()`, nest it inside another composition, or use
it in a pipeline. This is useful when you want to treat a group of checks as
one unit -- for example, bundling all your code checks into a single "strict
code" guardrail.

```{r compose-all-mode}
# Compose three code guardrails -- ALL must pass (default)
strict_code <- compose_guardrails(
  guard_code_analysis(),
  guard_code_complexity(max_ast_depth = 10, max_calls = 50),
  guard_code_dependencies(allowed_packages = c("dplyr", "ggplot2"))
)

strict_code

# Clean code passes all three
run_guardrail(strict_code, "dplyr::filter(mtcars, cyl == 4)")

# system() fails code analysis
run_guardrail(strict_code, "system('whoami')")

# processx fails dependency check
run_guardrail(strict_code, "processx::run('ls')")
```

### mode = "any": At Least One Must Pass

The default `mode = "all"` is the right choice for security checks: all
guards must pass. But sometimes you need the opposite logic -- an allowlist
where the input is acceptable if it matches **any** of several categories.
With `mode = "any"`, the composite passes if at least one child guardrail
passes:

```{r compose-any-mode}
# Accept prompts about either statistics OR machine learning
topic_guard <- compose_guardrails(
  guard_topic_scope(allowed_topics = c("statistics", "regression", "t-test")),
  guard_topic_scope(allowed_topics = c("machine learning", "neural network")),
  mode = "any"
)

run_guardrail(topic_guard, "How do I run a t-test in R?")
run_guardrail(topic_guard, "Explain neural network backpropagation")
run_guardrail(topic_guard, "What is the weather today?")
```

### check_all(): Run a List and Collect Results

Sometimes you need individual results from each guardrail rather than a single
composite result. `check_all()` runs a list of guardrails and returns a
summary:

```{r check-all}
guards <- list(
  guard_code_analysis(),
  guard_code_complexity(max_ast_depth = 10),
  guard_code_dataflow()
)

result <- check_all(guards, "x <- mean(1:10)")
result$pass
length(result$results)  # one per guardrail

# Inspect individual results
vapply(result$results, function(r) r@pass, logical(1))
```

When a check fails, `check_all()` collects all failure reasons:

```{r check-all-fail}
result <- check_all(guards, "Sys.getenv('SECRET_KEY')")
result$pass
result$reasons
```

### When to Use compose_guardrails() vs check_all()

Both functions combine multiple guardrails, but they serve different purposes
and return different types:

**Use `compose_guardrails()`** when you want a single guardrail object that
you can pass to `run_guardrail()`, nest inside another `compose_guardrails()`,
or use in a `secure_pipeline()`. The composed guardrail behaves as one unit:
you get a single pass/fail result. This is the right choice when you are
building reusable guardrail configurations (e.g., a "strict code" composite)
that you want to treat as a single check.

**Use `check_all()`** when you need diagnostic detail. It returns individual
results for each guardrail in the list, so you can report exactly which checks
failed and why. This is useful in logging, debugging, and user-facing error
messages where "code guardrail failed" is less helpful than "blocked function
`system()` detected by code_analysis; exceeded max AST depth of 10 per
code_complexity."

In practice, many applications use both: `compose_guardrails()` to build
reusable guardrail groups, and `check_all()` at the top level to get per-group
diagnostics.

## Building Pipelines with secure_pipeline()

Individual guardrails and compositions are useful for targeted checks, but a
production agent needs all three defense layers working together. A pipeline
bundles guardrails for input, code, and output into a single object with
dedicated methods for each stage. This ensures consistent configuration --
you define your security policy once and apply it uniformly across every
agent turn.

### Defining a Pipeline

```{r pipeline-create}
pipeline <- secure_pipeline(
  input_guardrails = list(
    guard_prompt_injection(sensitivity = "high"),
    guard_input_pii(),
    guard_topic_scope(allowed_topics = c("statistics", "data analysis", "R"))
  ),
  code_guardrails = list(
    guard_code_analysis(),
    guard_code_complexity(max_ast_depth = 15, max_calls = 100),
    guard_code_dependencies(allowed_packages = c("dplyr", "ggplot2", "tidyr")),
    guard_code_dataflow(block_network = TRUE, block_file_write = TRUE)
  ),
  output_guardrails = list(
    guard_output_pii(),
    guard_output_secrets(action = "redact"),
    guard_output_size(max_chars = 10000, max_lines = 200)
  )
)
```

### Running Each Stage

```{r pipeline-input}
# Stage 1: validate user input
input_result <- pipeline$check_input("Calculate the mean and sd of mtcars$mpg")
input_result$pass
```

```{r pipeline-code}
# Stage 2: validate LLM-generated code
code_result <- pipeline$check_code("
  library(dplyr)
  mtcars %>%
    summarise(mean_mpg = mean(mpg), sd_mpg = sd(mpg))
")
code_result$pass
```

```{r pipeline-output}
# Stage 3: filter execution output
output_result <- pipeline$check_output("mean_mpg = 20.09, sd_mpg = 6.03")
output_result$pass
output_result$result  # possibly redacted text
```

### Pipeline in an Agent Loop

The three `check_*` methods are designed to be called sequentially in an
agent loop. Each stage short-circuits on failure: if `check_input()` rejects
the prompt, you skip the LLM call entirely, saving both time and cost. If
`check_code()` rejects the generated code, you skip execution, preventing
any side effects. Here is the complete pattern:

```{r agent-loop, eval = FALSE}
process_turn <- function(pipeline, user_prompt, llm_fn, execute_fn) {
  # 1. Input guardrails

  input_check <- pipeline$check_input(user_prompt)
  if (!input_check$pass) {
    return(list(
      success = FALSE,
      stage = "input",
      reasons = input_check$reasons
    ))
  }

  # 2. LLM generates code
  code <- llm_fn(user_prompt)

  # 3. Code guardrails
  code_check <- pipeline$check_code(code)
  if (!code_check$pass) {
    return(list(
      success = FALSE,
      stage = "code",
      reasons = code_check$reasons
    ))
  }

  # 4. Execute in sandbox
  result <- execute_fn(code)

  # 5. Output guardrails
  output_check <- pipeline$check_output(result)
  if (!output_check$pass) {
    return(list(
      success = FALSE,
      stage = "output",
      reasons = output_check$reasons
    ))
  }

  list(success = TRUE, result = output_check$result)
}
```

## Mixing Custom and Built-In Guardrails

One of the key design principles in secureguard is that custom and built-in
guardrails share the same interface. This means custom guardrails compose
seamlessly with built-in ones -- you can mix them freely in
`compose_guardrails()`, `check_all()`, and `secure_pipeline()`. There is no
special registration step or plugin system; if it is a `secureguard` object,
it works everywhere:

```{r mix-custom}
# The SQL injection guard from earlier alongside built-in input guards
input_guards <- compose_guardrails(
  guard_prompt_injection(),
  guard_input_pii(),
  guard_sql_injection()
)

run_guardrail(input_guards, "Please help me write a SELECT query")
run_guardrail(input_guards, "' OR 1=1 --")
```

Similarly for code guardrails:

```{r mix-custom-code}
# Custom length guard composed with built-in code guards
code_guards <- compose_guardrails(
  guard_code_analysis(),
  guard_code_complexity(max_ast_depth = 10),
  guard_code_length(max_lines = 50)
)

run_guardrail(code_guards, "x <- mean(1:10)")
```

## Integration with securer

secureguard provides semantic analysis of code and outputs -- it understands
what the code does. [securer](https://github.com/ian-flores/securer) provides
OS-level sandboxing -- it limits what the code **can** do, regardless of
what it tries. Together they form two independent defense layers: secureguard
catches known-dangerous patterns before execution, and securer contains
unknown threats at the operating system level.

securer is a suggested dependency -- all of the patterns above work without
it. The integration layer adds two capabilities: pre-execution hooks and
output guarding after execution.

### Pre-Execute Hooks

`as_pre_execute_hook()` converts code guardrails into a function that securer
calls before executing each code snippet. It returns `TRUE` to allow execution
or `FALSE` to block it.

```{r pre-execute-hook, eval = FALSE}
library(securer)
library(secureguard)

hook <- as_pre_execute_hook(
  guard_code_analysis(),
  guard_code_complexity(max_ast_depth = 15),
  guard_code_dataflow()
)

sess <- SecureSession$new(pre_execute_hook = hook)
sess$execute("mean(1:10)")        # allowed
sess$execute("system('whoami')")  # blocked by code_analysis
sess$execute("Sys.getenv('KEY')") # blocked by dataflow
sess$close()
```

### Post-Execute Output Guarding

`guard_output()` runs output guardrails on execution results. Guardrails with
`action = "redact"` transform the output rather than blocking it:

```{r guard-output, eval = FALSE}
result <- sess$execute("paste('My API key is', 'AKIAIOSFODNN7EXAMPLE')")

checked <- guard_output(
  result,
  guard_output_pii(),
  guard_output_secrets(action = "redact")
)

if (checked$pass) {
  # Return the (possibly redacted) result to the user
  checked$result
} else {
  paste("Blocked:", paste(checked$reasons, collapse = "; "))
}
```

### Pipeline Hook

A pipeline can produce a pre-execute hook from its code guardrails:

```{r pipeline-hook, eval = FALSE}
pipeline <- secure_pipeline(
  input_guardrails = list(guard_prompt_injection()),
  code_guardrails = list(
    guard_code_analysis(),
    guard_code_dataflow()
  ),
  output_guardrails = list(
    guard_output_secrets(action = "redact")
  )
)

sess <- SecureSession$new(
  pre_execute_hook = pipeline$as_pre_execute_hook()
)

# The session now has code guardrails enforced automatically.
# Input and output guardrails are checked manually:
input_check <- pipeline$check_input(user_prompt)
# ... LLM generates code, session executes it ...
output_check <- pipeline$check_output(execution_result)

sess$close()
```

## Advanced Composition Patterns

The patterns above cover the common case: apply the same guardrails uniformly
to every request. In practice, you often need to vary guardrail strictness
based on context -- who the user is, where the request came from, and what
level of trust is appropriate. The following patterns show how to build
adaptive guardrail configurations.

### Layered Sensitivity

Not all contexts require the same level of strictness. A public-facing chatbot
is exposed to adversarial users and needs high-sensitivity injection detection
and tight topic scoping. An internal analytics tool used by trusted data
scientists can afford lower sensitivity to avoid false positives on legitimate
analytical prompts:

```{r layered-sensitivity}
# Public-facing: high sensitivity, strict topic scoping
public_guards <- compose_guardrails(
  guard_prompt_injection(sensitivity = "high"),
  guard_input_pii(),
  guard_topic_scope(allowed_topics = c("data analysis", "statistics"))
)

# Internal tool: lower sensitivity, broader topics
internal_guards <- compose_guardrails(
  guard_prompt_injection(sensitivity = "low"),
  guard_input_pii()
)

run_guardrail(
  public_guards,
  "Continue from where we left off with the regression"
)

run_guardrail(
  internal_guards,
  "Continue from where we left off with the regression"
)
```

### Graduated Code Restrictions

The same principle applies to code guardrails. A trusted internal user running
vetted analysis scripts needs fewer restrictions than an untrusted external
user whose prompts generate arbitrary code. By defining multiple guardrail
configurations and selecting one based on context, you can balance security
with usability:

```{r graduated-code}
# Trusted context: only block the most dangerous operations
trusted_code <- compose_guardrails(
  guard_code_analysis(blocked_functions = c("system", "system2", "shell")),
  guard_code_dataflow(
    block_env_access = TRUE,
    block_network = FALSE,
    block_file_write = FALSE
  )
)

# Untrusted context: strict lockdown
untrusted_code <- compose_guardrails(
  guard_code_analysis(),
  guard_code_complexity(max_ast_depth = 10, max_calls = 30),
  guard_code_dependencies(allowed_packages = c("dplyr", "ggplot2")),
  guard_code_dataflow(
    block_env_access = TRUE,
    block_network = TRUE,
    block_file_write = TRUE,
    block_file_read = TRUE
  )
)

# The same code may pass in trusted but fail in untrusted
code <- "readLines('data.csv')"
run_guardrail(trusted_code, code)
run_guardrail(untrusted_code, code)
```

### Redact vs Block Decision

Not all sensitive data in output requires the same response. PII like social
security numbers or patient records should block the entire response -- partial
disclosure is still a privacy violation. But API keys and tokens can often be
redacted in place, preserving the useful parts of the response while removing
the sensitive value. Output guardrails support three actions (`"block"`,
`"redact"`, `"warn"`) that let you make this distinction explicitly:

```{r redact-vs-block}
# PII blocks the output entirely
# Secrets get redacted so the response is still useful
pipeline <- secure_pipeline(
  output_guardrails = list(
    guard_output_pii(),                         # blocks on PII
    guard_output_secrets(action = "redact"),     # redacts secrets
    guard_output_size(max_chars = 5000)          # blocks oversized output
  )
)

# Secrets are redacted, not blocked
result <- pipeline$check_output("API key: AKIAIOSFODNN7EXAMPLE, data looks good")
result$pass
result$result

# PII causes a block
result <- pipeline$check_output("Patient SSN: 123-45-6789")
result$pass
result$reasons
```

## Summary

| Pattern | Function | Use Case |
|---------|----------|----------|
| Custom guardrail | `new_guardrail()` | Domain-specific checks |
| Same-type composition | `compose_guardrails()` | Merge guards into one reusable unit |
| Batch check | `check_all()` | Individual results per guard (diagnostics) |
| Full pipeline | `secure_pipeline()` | Three-layer defense for production |
| Pre-execute hook | `as_pre_execute_hook()` | securer integration |
| Output guard | `guard_output()` | Post-execution filtering |

The key design principle is that guardrails are composable values. Build small,
focused guards that each address one threat. Compose them for your context using
`compose_guardrails()` or `check_all()`. Assemble them into pipelines that
protect every stage of an agent workflow. And when the built-in guardrails do
not cover your domain, extend the system with `new_guardrail()` -- custom
guards compose identically to built-in ones.
